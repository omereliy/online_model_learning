#!/usr/bin/env python
"""
Visualize precision/recall evolution from checkpoint metrics.

This script creates visualizations from the checkpoint_metrics.csv file
generated by recalculate_model_metrics.py, showing how model quality
evolves over time for both safe and complete models.

Usage:
    python scripts/visualize_checkpoint_metrics.py results/paper/comparison_*/
    python scripts/visualize_checkpoint_metrics.py --latest
"""

import argparse
import matplotlib.pyplot as plt
import pandas as pd
from pathlib import Path
import logging
import sys

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent))

logger = logging.getLogger(__name__)

# Setup plotting style for publication
try:
    plt.style.use('seaborn-v0_8-whitegrid')
except:
    plt.style.use('ggplot')  # Fallback if seaborn style not available

plt.rcParams['figure.dpi'] = 150
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['font.size'] = 10
plt.rcParams['legend.fontsize'] = 9
plt.rcParams['axes.titlesize'] = 11
plt.rcParams['axes.labelsize'] = 10

# Algorithm display names
ALGORITHM_LABELS = {
    "olam": "OLAM",
    "information_gain": "Information Gain"
}

# Colors for consistency
COLORS = {
    "olam": "#1f77b4",
    "information_gain": "#ff7f0e"
}


def plot_precision_recall_evolution(df: pd.DataFrame, output_dir: Path,
                                   title_suffix: str = "") -> None:
    """
    Plot precision and recall evolution over iterations.

    Creates 3 plots:
    - Precondition precision/recall
    - Add effect precision/recall
    - Delete effect precision/recall

    Args:
        df: DataFrame with checkpoint metrics
        output_dir: Directory to save plots
        title_suffix: Suffix for plot titles (e.g., " - Blocksworld")
    """
    components = [
        ("precondition", "Preconditions"),
        ("add_effect", "Add Effects"),
        ("del_effect", "Delete Effects")
    ]

    for component_key, component_title in components:
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

        # Plot safe model
        for algorithm in df["algorithm"].unique():
            algo_label = ALGORITHM_LABELS.get(algorithm, algorithm)
            color = COLORS.get(algorithm, None)

            # Filter data for this algorithm and safe model
            data = df[(df["algorithm"] == algorithm) & (df["model_type"] == "safe")]

            if not data.empty:
                # Group by iteration and take mean across domains/problems
                grouped = data.groupby("iteration").mean(numeric_only=True)

                ax1.plot(grouped.index, grouped[f"{component_key}_precision"],
                        label=f"{algo_label} (Precision)", marker='o',
                        color=color, linestyle='-', alpha=0.8, markersize=4)
                ax1.plot(grouped.index, grouped[f"{component_key}_recall"],
                        label=f"{algo_label} (Recall)", marker='s',
                        color=color, linestyle='--', alpha=0.8, markersize=4)

        ax1.set_title(f"Safe Model - {component_title}{title_suffix}")
        ax1.set_xlabel("Iteration")
        ax1.set_ylabel("Score")
        ax1.set_ylim([0, 1.05])
        ax1.legend(loc='lower right')
        ax1.grid(True, alpha=0.3)

        # Plot complete model
        for algorithm in df["algorithm"].unique():
            algo_label = ALGORITHM_LABELS.get(algorithm, algorithm)
            color = COLORS.get(algorithm, None)

            # Filter data for this algorithm and complete model
            data = df[(df["algorithm"] == algorithm) & (df["model_type"] == "complete")]

            if not data.empty:
                # Group by iteration and take mean across domains/problems
                grouped = data.groupby("iteration").mean(numeric_only=True)

                ax2.plot(grouped.index, grouped[f"{component_key}_precision"],
                        label=f"{algo_label} (Precision)", marker='o',
                        color=color, linestyle='-', alpha=0.8, markersize=4)
                ax2.plot(grouped.index, grouped[f"{component_key}_recall"],
                        label=f"{algo_label} (Recall)", marker='s',
                        color=color, linestyle='--', alpha=0.8, markersize=4)

        ax2.set_title(f"Complete Model - {component_title}{title_suffix}")
        ax2.set_xlabel("Iteration")
        ax2.set_ylabel("Score")
        ax2.set_ylim([0, 1.05])
        ax2.legend(loc='lower right')
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        filename = f"{component_key}_evolution{title_suffix.replace(' ', '_').lower()}.pdf"
        output_path = output_dir / filename
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"Saved {output_path}")


def plot_f1_comparison(df: pd.DataFrame, output_dir: Path,
                       title_suffix: str = "") -> None:
    """
    Plot F1 score comparison for safe vs complete models.

    Args:
        df: DataFrame with checkpoint metrics
        output_dir: Directory to save plots
        title_suffix: Suffix for plot titles
    """
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    components = [
        ("precondition_f1", "Precondition F1"),
        ("add_effect_f1", "Add Effect F1"),
        ("del_effect_f1", "Delete Effect F1"),
        ("overall_f1", "Overall F1")
    ]

    for ax, (component_key, component_title) in zip(axes.flat, components):
        for algorithm in df["algorithm"].unique():
            algo_label = ALGORITHM_LABELS.get(algorithm, algorithm)
            color = COLORS.get(algorithm, None)

            # Safe model data
            safe_data = df[(df["algorithm"] == algorithm) & (df["model_type"] == "safe")]
            if not safe_data.empty:
                safe_grouped = safe_data.groupby("iteration").mean(numeric_only=True)
                ax.plot(safe_grouped.index, safe_grouped[component_key],
                       label=f"{algo_label} (Safe)", marker='o',
                       color=color, linestyle='--', alpha=0.7, markersize=4)

            # Complete model data
            complete_data = df[(df["algorithm"] == algorithm) & (df["model_type"] == "complete")]
            if not complete_data.empty:
                complete_grouped = complete_data.groupby("iteration").mean(numeric_only=True)
                ax.plot(complete_grouped.index, complete_grouped[component_key],
                       label=f"{algo_label} (Complete)", marker='s',
                       color=color, linestyle='-', alpha=0.9, markersize=4)

        ax.set_title(f"{component_title}{title_suffix}")
        ax.set_xlabel("Iteration")
        ax.set_ylabel("F1 Score")
        ax.set_ylim([0, 1.05])
        ax.legend(loc='lower right')
        ax.grid(True, alpha=0.3)

    plt.tight_layout()
    filename = f"f1_comparison_safe_vs_complete{title_suffix.replace(' ', '_').lower()}.pdf"
    output_path = output_dir / filename
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    logger.info(f"Saved {output_path}")


def plot_learning_curves(df: pd.DataFrame, output_dir: Path) -> None:
    """
    Plot learning curves showing overall F1 evolution for each algorithm.

    Args:
        df: DataFrame with checkpoint metrics
        output_dir: Directory to save plots
    """
    fig, ax = plt.subplots(figsize=(10, 6))

    for algorithm in df["algorithm"].unique():
        algo_label = ALGORITHM_LABELS.get(algorithm, algorithm)
        color = COLORS.get(algorithm, None)

        # Use complete model for main comparison
        data = df[(df["algorithm"] == algorithm) & (df["model_type"] == "complete")]

        if not data.empty:
            # Group by iteration and calculate mean and std
            grouped = data.groupby("iteration")["overall_f1"]
            mean_f1 = grouped.mean()
            std_f1 = grouped.std()

            # Plot mean with confidence interval
            ax.plot(mean_f1.index, mean_f1.values,
                   label=algo_label, marker='o',
                   color=color, linewidth=2, markersize=5)

            # Add shaded confidence interval
            ax.fill_between(mean_f1.index,
                          mean_f1.values - std_f1.values,
                          mean_f1.values + std_f1.values,
                          color=color, alpha=0.2)

    ax.set_title("Learning Curves - Overall F1 Score (Complete Model)")
    ax.set_xlabel("Iteration")
    ax.set_ylabel("F1 Score")
    ax.set_ylim([0, 1.05])
    ax.legend(loc='lower right')
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    output_path = output_dir / "learning_curves.pdf"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    logger.info(f"Saved {output_path}")


def create_summary_table(df: pd.DataFrame, output_dir: Path) -> None:
    """
    Create a summary table of final metrics at different checkpoints.

    Args:
        df: DataFrame with checkpoint metrics
        output_dir: Directory to save table
    """
    # Select key checkpoints
    key_iterations = [10, 50, 100, 200, 400]
    available_iterations = sorted(df["iteration"].unique())
    selected_iterations = [it for it in key_iterations if it in available_iterations]

    if not selected_iterations:
        logger.warning("No key iterations found in data")
        return

    summary_data = []

    for algorithm in df["algorithm"].unique():
        for model_type in ["safe", "complete"]:
            for iteration in selected_iterations:
                data = df[(df["algorithm"] == algorithm) &
                         (df["model_type"] == model_type) &
                         (df["iteration"] == iteration)]

                if not data.empty:
                    summary_data.append({
                        "Algorithm": ALGORITHM_LABELS.get(algorithm, algorithm),
                        "Model": model_type.capitalize(),
                        "Iteration": iteration,
                        "Prec. P": f"{data['precondition_precision'].mean():.3f}",
                        "Prec. R": f"{data['precondition_recall'].mean():.3f}",
                        "Add P": f"{data['add_effect_precision'].mean():.3f}",
                        "Add R": f"{data['add_effect_recall'].mean():.3f}",
                        "Del P": f"{data['del_effect_precision'].mean():.3f}",
                        "Del R": f"{data['del_effect_recall'].mean():.3f}",
                        "F1": f"{data['overall_f1'].mean():.3f}"
                    })

    if summary_data:
        summary_df = pd.DataFrame(summary_data)

        # Save as CSV
        csv_path = output_dir / "checkpoint_summary.csv"
        summary_df.to_csv(csv_path, index=False)
        logger.info(f"Saved summary table to {csv_path}")

        # Display
        print("\n" + "="*80)
        print("CHECKPOINT METRICS SUMMARY")
        print("="*80)
        print(summary_df.to_string(index=False))
        print("="*80)


def find_latest_comparison_dir() -> Path:
    """Find the latest comparison directory."""
    results_dir = Path("results/paper")
    comparison_dirs = list(results_dir.glob("comparison_*"))
    if not comparison_dirs:
        raise ValueError("No comparison directories found in results/paper/")
    return max(comparison_dirs)


def main():
    parser = argparse.ArgumentParser(
        description="Visualize checkpoint metrics for model learning comparison"
    )
    parser.add_argument("results_dir", type=Path, nargs='?',
                       help="Results directory containing checkpoint_metrics.csv")
    parser.add_argument("--latest", action="store_true",
                       help="Use latest comparison directory")
    parser.add_argument("--verbose", action="store_true",
                       help="Verbose output")

    args = parser.parse_args()

    # Configure logging
    logging.basicConfig(
        level=logging.DEBUG if args.verbose else logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    # Find results directory
    if args.latest or not args.results_dir:
        try:
            results_dir = find_latest_comparison_dir()
            logger.info(f"Using latest comparison directory: {results_dir}")
        except ValueError as e:
            logger.error(str(e))
            sys.exit(1)
    else:
        results_dir = args.results_dir

    # Check for checkpoint metrics file
    metrics_csv = results_dir / "checkpoint_metrics.csv"
    if not metrics_csv.exists():
        logger.error(f"Checkpoint metrics file not found: {metrics_csv}")
        logger.info("Please run: python scripts/recalculate_model_metrics.py first")
        sys.exit(1)

    # Load data
    logger.info(f"Loading metrics from {metrics_csv}")
    df = pd.read_csv(metrics_csv)
    logger.info(f"Loaded {len(df)} rows of metrics data")

    # Create visualizations directory
    viz_dir = results_dir / "checkpoint_visualizations"
    viz_dir.mkdir(exist_ok=True)

    # Generate overall plots
    logger.info("Generating overall precision/recall evolution plots...")
    plot_precision_recall_evolution(df, viz_dir)

    logger.info("Generating F1 comparison plots...")
    plot_f1_comparison(df, viz_dir)

    logger.info("Generating learning curves...")
    plot_learning_curves(df, viz_dir)

    # Generate domain-specific plots
    domains = df["domain"].unique()
    if len(domains) > 1:
        logger.info("Generating domain-specific plots...")
        for domain in domains:
            domain_df = df[df["domain"] == domain]
            domain_dir = viz_dir / f"domain_{domain}"
            domain_dir.mkdir(exist_ok=True)

            plot_precision_recall_evolution(domain_df, domain_dir,
                                          title_suffix=f" - {domain.title()}")
            plot_f1_comparison(domain_df, domain_dir,
                             title_suffix=f" - {domain.title()}")

    # Create summary table
    logger.info("Creating summary table...")
    create_summary_table(df, viz_dir)

    logger.info(f"\nAll visualizations saved to: {viz_dir}")
    logger.info("Visualization complete!")


if __name__ == "__main__":
    main()